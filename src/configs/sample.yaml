general:
  seed: 57
  epoch: 100
  gpus: [0]
  precision: 16
  amp_backend: "native" # [native or apex]
  amp_level: O2
  accelerator: null # [dp, ddp, ddp_cpu or ddp2]
  acc_grad: 1
  deterministic: True
  debug: False
  limit_val_batches: 0.25
  resume_from_checkpoint: null

data:
  dataset:
    root: ../data/CIFAR10/train
    k_fold: 5
    val_k: 0
  dataloader:
    batch_size: 1024
    num_workers: 4
    shuffle: True
    pin_memory: True

model:
  base: timm
  model_name: tf_efficientnet_b4
  pretrained: True
  num_classes: 10
  in_chans: 3

loss:
  base: torch
  name: cross_entropy

optimizer:
  optimizer:
    base: torch_optimizer
    optim_name: RAdam
    params:
      lr: !!float 1e-2
  scheduler:
    base: torch
    sche_name: CosineAnnealingLR
    params:
      T_max: 50
      eta_min: !!float 1e-5

callback:
  checkpoint:
    monitor: val_loss_mean
    save_last: True
    save_top_k: 1
    mode: min
    save_weights_only: True
    filename: "{epoch}-{val_loss_mean:.3f}-{val_acc:.3f}.pth"

transform:
  train:
    Compose:
      - RandomResizedCrop:
          height: 32
          width: 32
          scale: [0.98, 1.0]
          ratio: [0.98, 1.0]
      - Flip
      - Normalize:
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]
      - ToTensorV2

  val:
    Compose:
      - Normalize:
          mean: [0.485, 0.456, 0.406]
          std: [0.229, 0.224, 0.225]
      - ToTensorV2
